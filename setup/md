#!/bin/sh
# - raid_level
# - raid_chunk

: ${raid_level:=JBOD}
: ${raid_chunk:=256}

. $LKP_SRC/lib/wipefs.sh

check_raid_level()
{
	[ "${raid_level#*raid}" != "$raid_level" ] || exit 0
	[ "${raid_level#*braid}" != "$raid_level" ] && exit 0
}
check_raid_level

check_partition_size() {
	# though we can set 'show_part=1' for brd to make those ram disk to
	# show up at /proc/partitions, it's new parameter added lately, hence
	# we still meet troubles with old kernele. Here workaround it.
	[ "${disk_description#*brd}" != "$disk_description" ] && return
	for dev do
		[ "${dev#*:/}" != "$dev" ] && continue

		local part="$(basename $dev)"
		local size="$(grep -w $part /proc/partitions | awk '{ print $3 }')"
		[ -n "$size" ] || {
			echo "failed to get partition size for $dev" >&2
			exit 1
		}
		if [ -n "$prev_size" ]; then
			[ "$size" != "$prev_size" ] || {
				echo "non-equal partition size: $dev $size" >&2
				cat /proc/partitions >&2
				exit 1
			}
		else
			local prev_size=size
		fi
	done
}
check_partition_size $partitions

make_md() {
	if [ -e /proc/mdstat ]; then
		grep -q '^md0 : active' /proc/mdstat &&
		log_cmd mdadm -q --stop /dev/md0
	else
		log_cmd modprobe md
	fi

	destroy_devices

	for i in 0 1 2 3 4 5; do
		raid_device=/dev/md$i
		[ -b $raid_device ] && continue

		echo y | log_cmd mdadm -q				\
					--create $raid_device		\
					--chunk=$raid_chunk		\
					--level=$raid_level		\
					--raid-devices=$nr_partitions	\
					--force				\
					--assume-clean $partitions	\
					2> $TMP/mdadm_stderr && return
	done

	echo "Failed to create RAID!" 1>&2
	cat $TMP/mdadm_stderr 1>&2
	echo \
	cat /proc/mdstat
	cat /proc/mdstat
	exit 1
}

make_md

cat >> $TMP/env.yaml <<EOF

# setup/md
nr_partitions: 1
partitions: $raid_device
raid_device: $raid_device
raid_level: $raid_level
raid_chunk: $raid_chunk
EOF
